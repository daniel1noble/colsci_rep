# Reproducible research in biological colour-science

Thomas E. White^2,6^, Rhiannon L. Dalrymple^1,6^, Daniel W. A. Noble^2,6^, James C. O’Hanlon ^2,6^, Daniel B. Zurek^5,6^, Kate D. L. Umbers^3,4,6^

^1^ Evolution & Ecology Research Centre, School of Biological, Earth and Environmental Sciences, University of New South Wales, Kensington, NSW, Australia, 2022 ^2^ Department of Biological Sciences, Macquarie University, North Ryde, NSW, Australia, 2109 ^3^ School of Biological Sciences, University of Wollongong, Wollongong, NSW, Australia 2252 ^4^ Centre for Evolutionary Biology, University of Western Australia, Crawley, WA, Australia 6008 ^5^ Department of Biological Sciences, University of Pittsburgh, Pittsburgh, PA, USA, 15260

^6^ Authors contributed equally and are presented in random order

Corresponding Author:

Keywords: #colsci, spectrometry, photography, visual ecology, pigment, structural colour, visual modelling, methodology 

Article type: Commentary

Word count:

Number of figures: 

Number of tables: 2

Data archiving:

\newpage

## Introduction

Studies of biological colouration have generated insights into fundamental evolutionary and ecological processes, and research into colour traits is a rapidly growing field [@kelber_spectral_2010]. The resurgent interest in biological colouration has in large part been driven by the increased availability of key technologies, including spectrometry and photography, and concurrent advances in methods for analysing colour data, such as visual models [@endler_comparing_2005; @stevens_using_2007; @kelber_animal_2003]. While these developments are positive for the field, the increasingly complex analyses being run on ever greater amounts of data heighten the need for comprehensive methods reporting and proper data management [@alsheikh_public_2011; @nekrutenko_next_2012].      

Replication and transparency lie at the heart of the scientific enterprise. Beyond simply allowing the independent verification of results, reproducible research engenders greater comparability between studies and provides a foundation for the testing of new ideas and methods [@whitlock_data_2011; @van_trouble_2011; @piwowar_sharing_2007]. A study may be considered truly reproducible when it satisfies three broad criteria: (i) methods are reported completely, (ii) data are publicly archived and available, and (iii) analysis code is made publicly available, such that the chain of modification of raw data is documented and preserved. While completely reproducible research [e.g. @fitzjohn_much_2014] is a laudable goal, the considerable demands it imposes on researchers means that it will often, in practice, be unattainable. Nevertheless, even partial reproducibility through the relatively simple practices of complete methods reporting and public data archiving is of tremendous value.     

Our aim here to explore the state of reproducibility in studies of biological colouration, and to outline ways in which it may be improved in line with points (i) - (iii) above. We begin by briefly outlining some of the most common methods for studying biological colouration, with an emphasis on the way in which their subtle complexity can dramatically shape results. We then present a set of guidelines for the comprehensive reporting of methods, and we analyse a subset of the literature against these criteria to assess the current state of methods reporting in colour science. We also assess the availability of publicly archived data and code, and suggest some useful tools for increasing the reproducibility of colour trait research.  

## Researcher degrees of freedom in colour analysis

todo: hack this back, make it more relevant to reproducibility

Generations of biologists have endeavoured to explain the mechanisms and functions of animal and plant colouration [@thayer_concealing_1909; @poulton_colours_1890; @wallace_natural_1891], and identifying best practices in measuring colour has been a great challenge. The application of spectrometry to questions of biological colouration revolutionized the field [@dyck_determination_1966]. Now relatively inexpensive and convenient, spectrometry allows for the rapid capture of quantitative data, and has been widely adopted as a standard in the field [@andersson_quantifying_2006]. 

However one drawback to this methodology, is the ease with which erroneous data can be collected if attention is not paid to finer details of spectrum capture. The angle from which spectra are taken [@santos_strong_2007], consistent calibration of light and dark standards, the distance between light source and surface, and the size, shape, glossiness or iridescence of the specimen can all contribute to variation in data and potentially bias results [@andersson_quantifying_2006; @kemp_ornamental_2008; @meadows_quantifying_2011; @schaefer_birds_2008]. Experience, guidance, consistency and objectivity (e.g. double-blind experimental design) are necessary to ensure data are repeatable and representative of the subject. Crucially, to ensure repeatability across studies, the conditions under which data were collected must be reported exactly.  

Complementary to spectrometry, digital photography is increasingly being used to quantify biological colouration [@stevens_using_2007]. High-resolution cameras are inexpensive,  and allow instantaneous, repeatable sampling of multiple colour patches [@mckay_use_2013]. The main challenge for gathering colour data from photographs involve ensuring standardization, such as linearisation and consistent lighting [@stevens_using_2007]. Also, most digital cameras used for the measurement of biological colouration are consumer products designed for the human visual system. Thus hardware modifications for ultraviolet and infra-red imaging are generally necessary [e.g. @stevens_colour_2013]. 

Expansion in the availability of objective methods for the measurement of colour has been matched by advances in theory and analysis. In particular, the development of visual models has produced significant insights into the early visual processing of colourful stimuli [@chittka_colour_1992; @endler_comparing_2005; @vorobyev_receptor_1998] and, in a few cases, how they may be perceived [@kelber_spectral_2010]. Visual models typically attempt to describe the subjective perception of colour information as a function of an object’s reflectance, the ambient illumination, and a receiver’s sensory system [e.g. @endler_comparing_2005; @vorobyev_receptor_1998]. These models enable researchers to move beyond purely quantitative comparisons of reflectance spectra and adopt more biologically relevant perspectives to refine and test hypotheses. 

Although relatively easy to implement, and potentially powerful, visual models are built on multiple assumptions about the way in which stimuli are visually processed that can dramatically shape the results of a given analysis [@lind_avian_2009; @pike_preserving_2012]. For example, Vorobyev et al.’s [-@vorobyev_receptor_1998; -@vorobyev_colour_2001] widely used receptor-noise model requires researchers to specify the form of receptor quantum catches (i.e. raw versus transformed), the use of any colour constancy mechanism (e.g. von Kries), as well as the photoreceptor signal-to-noise ratios, their relative densities, and the type of noise predominating (i.e. neural and/or receptor noise). These data are only available for a small number of species, such as humans and honeybees, and so assumptions and extrapolations are necessary when models are applied to other species. Unless the biological rationale underlying every parameter choice is clearly understood, justified, and reported, such analyses may become ineffective, especially when comparing results between studies [@pike_preserving_2012].  

The potential for subjectivity in colour measurement and the mathematical opacity of visual models for the average biologist means that, as in any field, is it critical that methods are reported rigorously. Methods reporting should ideally have well defined standards such that sufficient detail is provided to allow comprehensive peer review, precise replication, cross-study comparison and data sharing [@alsheikh_public_2011; @sandve_ten_2013].... 

## Assessing reproducibility

To assess the current state of reproducibility in the field we searched papers from 2013 in 22 leading journals (_American Journal of Botany_, _The American Naturalist_, _Animal Behaviour_, _Behavioral Ecology_, _Behavioral Ecology and Sociobiology_, _Biological Journal of the Linnean Society_, _Biology Letters_, _Current Zoology_, _Ecology_, _Ecology and Evolution_, _Ecology Letters_, _Ethology_, _Evolution_, _Functional Ecology_, _Journal of Ecology_, _The Journal of Evolutionary Biology_, _The Journal of Experimental Biology_, _Naturwissenschaften_, _New Phytologist_, _Oikos_, _PLoS ONE_, _Proceedings of the Royal Society B: Biological Sciences_). On each of the journals' homepages, we used the Boolean terms 'colour*', 'color*' or 'spectra*' to search the title or abstract. We read the titles and abstracts of each paper and included only those papers that used either a spectrometer or camera to quantify colouration. We excluded papers review papers, methodological papers, papers quantifying colour patterns rather than the chromatic properties of a colour patch, and papers taking micro-spectrometric measurements of retinal absorbance. To review the large number of papers returned from our search, journals were haphazardly divided up between the authors. In order to reduce the risk of observer bias in our assesment, each paper was read and reassessed by three authors. Any discrepancies between assessor scores were discussed and resolved prior to analysis. We then tabulated what details of the methods were reported against our criteria. The data along with analysis script can be found on github (WHAT'S THE DOI?). We have kept the papers used in our dataset anonymous as our aim is to explore the general question of repeatability in the field. Our search returned a total of 216 papers, however, only `r nrow(coldat)` met our criteria and were included in the final analysis.

## Methods reporting in colour studies 

_Guidelines for reporting_

The simplest step in ensuring research is reproducible is the comprehensive reporting of methods. The myriad researcher degrees of freedom in the capture and analyses of colour data (as outlined above) heighten this need. Accordingly, we developed a list of essential information about the capture (Table 1) and analysis (Table 2) of data that should be reported to ensure reproducibility. Given their overwhelming popularity, we focus on spectrometry and photography for data collection [e.g. @kemp_ornamental_2008; @chiao_visualization_2009], and do not consider qualitative methods such as the use of Munsell chips or paint swatches [e.g. @hogstad_gloger_2009]. 

Analytical techniques are diverse and are being developed rapidly [kelber_spectral_2010]. This rate of progression, however, means that a deep understanding of common methods is increasingly beyond the grasp of the average researcher. As a consequence, the subtle complexity of many analytical techniques [e.g. the 3-7 parameters that define a typical visual model; @endler_comparing_2005; @vorobyev_receptor_1998] is often overlooked by empiricists, which leads to critical methodological information often not being reported. Our guidelines for reporting analytical details (Table 2) cover two broad, common categories of analysis: colourimetric (or 'spectral') analyses, and visual modelling. These are essential if such analyses are to be reproduced. These criteria provided that baseline against which to hold our results. 

_The current state of methods reporting in the literature_

Overall, `r specs+both` studies used a spectrometer to measure colour, while `r cam+both` used a camera. Of these studies, `r both` quantified colour using both a spectrometer and a camera. Reporting of the make and model of the spectrometer and camera was high (Figure 1, spectrometers = `r round(specModel, digits = 2)`% and cameras = `r round(camModel, digits = 2)`%), however, only `r round(propCam_fin["prop_light_source"], digits = 2)`%  and `r round(propSpec_fin["prop_light_source"], digits = 2)`% of camera and spectrometer studies reported a light source. In addition, the number of reflectance spectra and pixels averaged were only reported in `r round(propSpec_fin["prop_specpix_avg"], digits = 2)`% and `r round(propCam_fin["prop_specpix_avg"], digits = 2)`% of studies, respectively. Surprisingly, dark standards were only reported in `r round(propSpec_fin["prop_dark_std"], digits = 2)`% of the spectrometer studies, while white standards were reported in `r round(propSpec_fin["prop_white_stdspec"], digits = 2)`%. Studies quantifying spectral reflectance often did not report the integration time of the spectrophotmeter (`r round(propSpec_fin["prop_int_time"], digits = 2)`%), or the angle of the optical probe relative to the surface (`r round(propSpec_fin["prop_spec_angle"], digits = 2)`%) or distance (`r round(propSpec_fin["prop_spec_dist"], digits = 2)`%) the probe was held from the measurement surface. 
	
The types of analysis varied across studies. Overall `r analysisType[1]` papers generated colourimetric variables [i.e. 'spectral' measures of hue, saturation, and/or brightness; @montgomerie_analyzing_2006] whereas `r analysisType[4]` used visual models, `r analysisType[2]` used both colourimetrics and visual models, and `r analysisType[3]` used other forms of analysis (e.g. purely statistical analyses, such as principle component analysis on reflectance spectra). Of the studies quantifying colour using spectral reflectance curves `r analysisTypeSpec[1]` used colourimetric measures, `r analysisTypeSpec[2]` used multiple analyses, `r analysisTypeSpec[4]` used visual models and `r analysisTypeSpec[3]` used other forms of analysis. Studies using colourimetric measurements defined their measure of brightness, hue and/or chroma in `r round(col_def, digits = 2)`% of the studies. 

Most studies using visual models analysed their data using receptor-noise models (`r vismod_type[3]` of `r sum(vismod_type)` studies), while others used the colour hexagon (`r vismod_type[1]` studies), or a tetrahedral colour space (`r vismod_type[4]` studies). Case specific models were used in `r vismod_type[2]` papers. Of visual modelling papers, `r round(propSpec_fin["prop_irrad_type"], digits =2)`% reported the type of irradiance when necessary, `r round(propSpec_fin["prop_vis_mod_sp"], digits = 2)`% the species modelled as a receiver. The background used in visual models was reported `r round(propSpec_fin["prop_vis_mod_bkg"], digits = 2)`% of the time. The type of receptor noise modelled was reported in only `r round(propSpec_fin["prop_vis_mod_noise_type"], digits = 2)`% of the studies. Explicit mention of the type of quantum catch (`r round(propSpec_fin["prop_vis_mod_qcatch"], digits = 2)`% of studies), or whether photoreceptor adaptation (`r round(propSpec_fin["prop_vis_mod_adapt"], digits = 2)`%) was modelled, was not common. 

While some of the percentages reported above seem alarming, it is important to note that around one-third of papers (`r round(ref_stud, digits = 2)`%) made reference to previous work for various methods. This means that our above figures are slightly liberal given that the referenced works may have comprehensively covered some of these criteria. Despite this, two-thirds of the studies did not reference previous work, and were missing important methodological details, suggesting the need for more comprehensive reporting of methods.  

## The public availability of data and code

Of the `r nrow(coldat)` studies analysed, (PERCENTAGE)% publicly provided the raw underlying data, (PERCENTAGE)% provided data in a pre-processed form, and (PERCENTAGE)% of studies did not provide any publicly accessible data. The paucity of data being made available post-publication in studies of biological colouration is in line with other fields [@vines_mandated_2013; @wolkovich_advances_2012], including the broader field of animal behaviour [@caetano_forgotten_2014]. There is evidence that this trend is gradually shifting as funders and journals increasingly mandate the release of data [@whitlock_data_2010]. The clearest benefit of open access to data is that it allows researchers to build upon previous work for the testing and refinement of new ideas and methods. This is particularly relevant to the field of colour-science, where new methods for analysing colour data are frequently developed [e.g. @allen_analyzing_2013; @endler_framework_2012; @stoddard_pattern_2014]. The provision of open data may also foster collaborations as researchers draw upon existing work, and increased data sharing has been shown to positively correlate with citations [@piwowar_sharing_2007]. Given the increasing rate of retractions [@steen_retractions_2010; @van_trouble_2011], there is also impetus for researchers to maintain credibility through transparency. We thus encourage researchers to publicly archive data in as raw a form as possible (e.g. individual reflectance spectra). Several excellent data repositories now exist, such as figshare and dryad, and the preparation of data for publication is relatively straightforward [@whitlock_data_2011]. The use of modest data embargoes and appropriate licenses can help to ensure that original authors are able to make the best use of data, and are subsequently credited [@neylon_science_2012].
 
None of the included studies linked to any form of code, which is not surprising given the ongoing popularity of graphical-user-interface based statistical [e.g. SPSS], and colour-analytical [e.g. avicol;] software. Benefits of code over point-and-click, maintain chain of mod for yourself and others, open source shit (R, pavo, things), github, sourcefourge etc. This paper was written openly on github, 
 
## Conclusions

Our review of recent colour literature highlights some important reporting gaps that will hinder the reproducibility and comparability of studies of biological colouration (Fig. 1). While we recognize that journals have space limitations and that work may reference previous methological and analytical details, we strongly believe that it is to the field's advantage that researchers report all details essential to a study's transparency and reproducibility wherever possible.  

The permanent private storage of data [or, more often, the complete loss of data; @alsheikh_public_2011] is increasingly difficult to justify, and the benefits of open data are considerable....

While all criteria are important, some deserve special attention. In particular, we found that the types of light sources, numbers of pixels or spectra averaged, and types of standards used (i.e. white and black reference samples) are under-reported, each of which is critical to repeatability and cross-comparison. Similarly, the distance and angle of the optical probe relative to the measurement surface and collector should be standard reporting for all studies using spectrometry, as variation in these factors can lead to dramatically different results in some cases (REFS). Visual models were regularly used to analyse spectral data, however, there were many important aspects of the analysis that were seldom reported. These included the type of quantum catch used, whether photoreceptor adaptation was modelled and the type of noise used in receptor-noise models. This is problematic because... Also, photoreceptor densities should be reported where possible even if using standard avian photoreceptor densities because.... These all have important consequences on downstream analysis and the reproducibility of colour signal quantification [e.g. @kemp_ornamental_2008; @santos_comparison_2007]. We also recommend the presentation of spectral reflectance curves where possible. These figures allow for the rapid assessment of the nature and quality of the data, independent of downstream processing (if the conditions under which they were captured is comprehensively reported. We found that only `r round(propSpec_fin["prop_refl_fig"], digits = 2)`% of studies using spectral reflectance data had a figure showing these curves. When possible, we also recommend that authors justify data capture and analytical approaches - something few studies we investigated explicitly outlined. 

Our objective in this paper was to highlight common oversights in reporting that can influence the reproducibility of colour research and cross-study comparisons. By no means do we wish to say that studies lacking important details are not of significant value, but rather to provide recommendations for future work. It is only through critical appraisal and collegial discussion that we can identify areas for improvement, and constructively contribute to the growth of our field.

We believe that part of the issue at hand is simply that many researchers seldom have clear guidelines to follow when reporting methods, and may overlook outlining important aspects of methodology and analysis. We hope our critical evaluation of recent literature has highlighted a need for more careful description of how colour signals have been measured and analysed. More importantly, we hope our guidelines for methods reporting helps establish a standard in this area, to continue the advance of an exciting era in colour research. 

## References